# stream of thoughts
### On reading ["One Weird Trick"](https://www.lesswrong.com/posts/fovfuFdpuEwQzJu2w/neural-networks-generalize-because-of-this-one-weird-trick)
#### 1
so 'one weird trick' says, "generalization is a balance between expressivity (more effective parameters) and simplicity (fewer effective parameters)."

and it clicked. this is the same idea i was wrestling with from the other article—the one about sharp peaks versus gentle hills on a probability distribution graph. it’s just a much more useful way to phrase it.

a sharp peak is what you get with high expressivity: tons of effective parameters trying to memorize every detail. a gentle hill is simplicity: fewer effective parameters, forcing the model to find the actual, simpler patterns in the data.

that trade-off, between capturing nuance and staying simple, is pretty much the entire game of making a neural network learn.

sigh. i definitely read these articles in the wrong order. shouldn't have switched them.
# to do
### 1
to watch and take notes on this video
[Singular Learning Theory - Seminar 20 - State of scaling laws](https://www.youtube.com/watch?v=7LzW8-wxdUE)

pitch: SLT may even enable us to construct a grand unified theory of scaling
### 2
to look throught (it is said to contain meaningfull alternative to SLT https://cclab.science/papers/ICLR_2019.pdf)
### 3
to code counterexamples from here https://www.lesswrong.com/posts/ALJYj4PpkqyseL7kZ/my-criticism-of-singular-learning-theory
# side ideas